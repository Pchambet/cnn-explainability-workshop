{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP ‚Äî Interpr√©tation des Filtres CNN & Grad-CAM\n",
    "**Master TRIED** ¬∑ Conf√©rence Ouverture Professionnelle ‚Äî Reconnaissance faciale avec VGG16\n",
    "\n",
    "> **Fil conducteur** : Les CNN sont souvent per√ßus comme des ¬´ bo√Ætes noires ¬ª. Ce TP d√©monte cette id√©e en visualisant ce que le r√©seau apprend (Partie 1), comment il prend ses d√©cisions (Partie 2), et comment exploiter ces connaissances pour construire un syst√®me pratique (Parties 3-4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 0. Environnement"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2, os\n",
    "\n",
    "# Reproductibilit√©\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# T√©l√©chargement automatique de l'image (pour Colab)\n",
    "if not os.path.exists(\"test_face.jpg\"):\n",
    "    print(\"üì• T√©l√©chargement de l'image de test...\")\n",
    "    os.system(\"wget https://raw.githubusercontent.com/Pchambet/cnn-explainability-workshop/main/test_face.jpg\")\n",
    "\n",
    "os.makedirs(\"output_figures\", exist_ok=True)\n",
    "print(f\"TensorFlow {tf.__version__} ¬∑ NumPy {np.__version__}\")\n",
    "print(f\"GPU: {tf.config.list_physical_devices('GPU') or 'CPU uniquement'}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Partie 1 ‚Äî Visualisation des Filtres par Maximisation des Activations\n",
    "\n",
    "## 1.1 Principe de l'algorithme\n",
    "\n",
    "La **maximisation des activations** (Feature Visualization) r√©pond √† une question simple mais profonde :\n",
    "\n",
    "> *Quel stimulus visuel maximise la r√©ponse d'un neurone donn√© ?*\n",
    "\n",
    "Formellement, on cherche l'image $x^*$ qui maximise l'activation $A_{l,f}$ d'un filtre $f$ dans la couche $l$ :\n",
    "\n",
    "$$x^* = \\arg\\max_x \\; \\frac{1}{HW} \\sum_{i,j} A_{l,f}(x)_{i,j}$$\n",
    "\n",
    "C'est un probl√®me d'**optimisation dans l'espace des pixels** (et non des poids). On part d'une image de bruit al√©atoire et on applique du **gradient ascent** :\n",
    "\n",
    "$$x_{t+1} = x_t + \\eta \\cdot \\frac{\\nabla_x A_{l,f}}{\\|\\nabla_x A_{l,f}\\|_2}$$\n",
    "\n",
    "La normalisation L2 du gradient est cruciale : elle stabilise l'optimisation en assurant un pas de taille constante, quelle que soit l'amplitude du gradient.\n",
    "\n",
    "> **Intuition** : C'est comme demander au r√©seau de ¬´ r√™ver ¬ª ‚Äî on lui fait g√©n√©rer l'image id√©ale qu'il associe √† un concept appris.\n",
    "\n",
    "> Ref: [Visualizing what convnets learn ‚Äî Keras](https://keras.io/examples/vision/visualizing_what_convnets_learn/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Chargement et analyse de VGG16"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Deux versions du mod√®le pour deux usages distincts :\n",
    "# 1) model : include_top=True (224√ó224 fixe) ‚Üí classification, Grad-CAM, pr√©dictions\n",
    "# 2) model_notop : include_top=False (taille flexible) ‚Üí visualisation des filtres\n",
    "#    (la visualisation g√©n√®re des images 128√ó128, incompatible avec l'entr√©e fixe 224√ó224)\n",
    "\n",
    "model = keras.applications.VGG16(weights='imagenet', include_top=True, input_shape=(224, 224, 3))\n",
    "model_notop = keras.applications.VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "# Analyse structurelle par bloc\n",
    "print(\"=\" * 70)\n",
    "print(\"VGG16 ‚Äî ARCHITECTURE PAR BLOCS\")\n",
    "print(\"=\" * 70)\n",
    "total_conv, total_fc = 0, 0\n",
    "for layer in model.layers:\n",
    "    cfg = layer.get_config()\n",
    "    if 'conv' in layer.name:\n",
    "        n = cfg.get('filters', 0)\n",
    "        total_conv += n\n",
    "        print(f\"  CONV  {layer.name:20s} | {n:>3} filtres | kernel {cfg.get('kernel_size')}\")\n",
    "    elif 'pool' in layer.name:\n",
    "        print(f\"  POOL  {layer.name:20s} | ‚Üì /2\")\n",
    "    elif 'dense' in layer.name or 'predictions' in layer.name:\n",
    "        u = cfg.get('units', 0)\n",
    "        total_fc += u\n",
    "        print(f\"  FC    {layer.name:20s} | {u:>5} unit√©s\")\n",
    "\n",
    "print(f\"\\nTotal conv filtres: {total_conv}\")\n",
    "print(f\"Total FC unit√©s:    {total_fc}\")\n",
    "print(f\"Total param√®tres:   {model.count_params():,}\")\n",
    "\n",
    "# Ratio param√®tres conv vs FC\n",
    "conv_params = sum(l.count_params() for l in model.layers if 'conv' in l.name)\n",
    "fc_params = sum(l.count_params() for l in model.layers if 'dense' in l.name or 'predictions' in l.name)\n",
    "print(f\"\\n‚ö†Ô∏è  Param√®tres FC: {fc_params:,} ({fc_params/model.count_params()*100:.1f}%) vs Conv: {conv_params:,} ({conv_params/model.count_params()*100:.1f}%)\")\n",
    "print(\"‚Üí Les couches FC repr√©sentent la majorit√© des param√®tres mais PAS la majorit√© de la connaissance visuelle.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation cl√© : le paradoxe des param√®tres VGG16\n",
    "\n",
    "Les couches **fully-connected** contiennent ~89% des param√®tres, mais ce sont les couches **convolutionnelles** (~11%) qui encodent la connaissance visuelle. Ce paradoxe explique pourquoi le **transfer learning** fonctionne : on r√©utilise les couches conv (extracteur de features universel) et on remplace les FC (classifieur sp√©cifique au domaine).\n",
    "\n",
    "**Architecture en pyramide invers√©e** :\n",
    "- R√©solution spatiale : 224‚Üí112‚Üí56‚Üí28‚Üí14‚Üí7 (‚Üì √ó32)\n",
    "- Nombre de filtres : 64‚Üí128‚Üí256‚Üí512‚Üí512 (‚Üë √ó8)\n",
    "- ‚Üí Le r√©seau **√©change de la r√©solution spatiale contre de la profondeur s√©mantique**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Visualisation des filtres"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def visualize_filter(layer_name, filter_index, size=128, steps=30, lr=10.0):\n",
    "    \"\"\"\n",
    "    G√©n√®re l'image qui maximise l'activation d'un filtre.\n",
    "    Utilise model_notop (entr√©e flexible) pour accepter des tailles arbitraires.\n",
    "    \"\"\"\n",
    "    extractor = keras.Model(\n",
    "        inputs=model_notop.input,\n",
    "        outputs=model_notop.get_layer(layer_name).output\n",
    "    )\n",
    "    image = tf.Variable(tf.random.uniform((1, size, size, 3)) * 0.25 + 0.5)\n",
    "    \n",
    "    for _ in range(steps):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(image)\n",
    "            activation = extractor(image)\n",
    "            # Crop les bords pour √©viter les artefacts de bord\n",
    "            filter_activation = activation[:, 2:-2, 2:-2, filter_index]\n",
    "            loss = tf.reduce_mean(filter_activation)\n",
    "        grads = tape.gradient(loss, image)\n",
    "        grads = tf.math.l2_normalize(grads)\n",
    "        image.assign_add(lr * grads)\n",
    "    \n",
    "    img = image[0].numpy()\n",
    "    img = (img - img.mean()) / (img.std() + 1e-5) * 0.15 + 0.5\n",
    "    return np.clip(img, 0, 1)\n",
    "\n",
    "# Validation rapide\n",
    "test = visualize_filter('block1_conv2', 0, steps=5)\n",
    "print(f\"‚úÖ Fonction op√©rationnelle ‚Äî shape: {test.shape}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation sur 3 niveaux hi√©rarchiques\n",
    "\n",
    "On visualise 8 filtres par couche √† 3 niveaux de profondeur pour observer la **hi√©rarchie des repr√©sentations** :\n",
    "\n",
    "| Niveau | Couche | Profondeur | Champ r√©ceptif th√©orique |\n",
    "|--------|--------|:----------:|:------------------------:|\n",
    "| Bas | `block1_conv2` | 2 conv | 5√ó5 px |\n",
    "| Interm√©diaire | `block3_conv3` | 8 conv | 44√ó44 px |\n",
    "| Haut | `block5_conv3` | 13 conv | 196√ó196 px |\n",
    "\n",
    "Le **champ r√©ceptif** croissant explique pourquoi les couches profondes d√©tectent des structures de plus en plus globales."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "layers = [\n",
    "    (\"block1_conv2\", \"Bas niveau\"),\n",
    "    (\"block3_conv3\", \"Interm√©diaire\"),\n",
    "    (\"block5_conv3\", \"Haut niveau\"),\n",
    "]\n",
    "N_FILTERS = 8\n",
    "\n",
    "fig, axes = plt.subplots(len(layers), N_FILTERS, figsize=(20, 8))\n",
    "for row, (name, label) in enumerate(layers):\n",
    "    print(f\"‚è≥ {name}...\")\n",
    "    for col in range(N_FILTERS):\n",
    "        img = visualize_filter(name, col)\n",
    "        axes[row, col].imshow(img)\n",
    "        axes[row, col].axis('off')\n",
    "        if col == 0:\n",
    "            axes[row, col].set_ylabel(label, fontsize=10, rotation=0, labelpad=130, fontweight='bold')\n",
    "\n",
    "plt.suptitle(\"Filtres VGG16 ‚Äî Maximisation des activations (3 niveaux)\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"output_figures/01_filters.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Analyse quantitative des filtres"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Mesurer la diversit√© et la qualit√© des filtres par couche\n",
    "def analyze_filters(layer_name, n_filters=16, size=64, steps=20):\n",
    "    \"\"\"Analyse statistique des filtres d'une couche.\"\"\"\n",
    "    images = []\n",
    "    for i in range(n_filters):\n",
    "        img = visualize_filter(layer_name, i, size=size, steps=steps)\n",
    "        images.append(img)\n",
    "    images = np.array(images)\n",
    "    \n",
    "    # 1) Entropie moyenne (complexit√© visuelle)\n",
    "    entropies = []\n",
    "    for img in images:\n",
    "        gray = np.mean(img, axis=2)\n",
    "        hist, _ = np.histogram(gray, bins=50)\n",
    "        hist = hist / (hist.sum() + 1e-10)  # Normalize to probabilities\n",
    "        hist = hist[hist > 0]\n",
    "        entropies.append(-np.sum(hist * np.log2(hist)))\n",
    "    \n",
    "    # 2) Corr√©lation inter-filtres (diversit√©)\n",
    "    flat = images.reshape(n_filters, -1)\n",
    "    corr_matrix = np.corrcoef(flat)\n",
    "    # Moyenne des corr√©lations hors-diagonale\n",
    "    mask = ~np.eye(n_filters, dtype=bool)\n",
    "    mean_corr = np.abs(corr_matrix[mask]).mean()\n",
    "    \n",
    "    # 3) Variance spatiale (structure vs bruit)\n",
    "    variances = [np.var(img) for img in images]\n",
    "    \n",
    "    return {\n",
    "        'entropy_mean': np.mean(entropies),\n",
    "        'entropy_std': np.std(entropies),\n",
    "        'inter_correlation': mean_corr,\n",
    "        'spatial_variance': np.mean(variances),\n",
    "        'n_analyzed': n_filters\n",
    "    }\n",
    "\n",
    "print(\"Analyse quantitative des filtres (peut prendre 2-3 min)...\")\n",
    "results = {}\n",
    "for name, label in [(\"block1_conv2\", \"Bas\"), (\"block3_conv3\", \"Mid\"), (\"block5_conv3\", \"Haut\")]:\n",
    "    print(f\"  ‚è≥ {name}...\")\n",
    "    results[name] = analyze_filters(name)\n",
    "\n",
    "# Affichage\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"{'Couche':20s} | {'Entropie':>10s} | {'Corr. inter':>12s} | {'Variance':>10s}\")\n",
    "print(\"-\" * 70)\n",
    "for name, r in results.items():\n",
    "    print(f\"{name:20s} | {r['entropy_mean']:>7.2f}¬±{r['entropy_std']:.2f} | {r['inter_correlation']:>11.4f} | {r['spatial_variance']:>10.6f}\")\n",
    "print(\"=\" * 70)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Discussion approfondie\n",
    "\n",
    "#### Hi√©rarchie des repr√©sentations ‚Äî Pourquoi c'est important\n",
    "\n",
    "Les r√©sultats confirment exp√©rimentalement une propri√©t√© fondamentale des CNNs profonds :\n",
    "\n",
    "**Couches basses (block1)** :\n",
    "- Entropie **faible** ‚Üí motifs simples et bien structur√©s (bords, gradients)\n",
    "- Corr√©lation inter-filtres **faible** ‚Üí chaque filtre capture un concept visuel distinct\n",
    "- Ces filtres sont des **d√©tecteurs de Gabor** appris ‚Äî ils red√©couvrent les primitives identifi√©es par les neurosciences (Hubel & Wiesel, 1962) dans le cortex visuel V1\n",
    "\n",
    "**Couches interm√©diaires (block3)** :\n",
    "- Entropie **croissante** ‚Üí motifs plus complexes (textures, motifs r√©p√©titifs)\n",
    "- Les filtres commencent √† **composer** les primitives des couches basses\n",
    "- Analogie biologique : aires V2/V4 du cortex visuel\n",
    "\n",
    "**Couches hautes (block5)** :\n",
    "- Entropie **maximale** ‚Üí structures tr√®s complexes, parfois difficiles √† interpr√©ter visuellement\n",
    "- Corr√©lation inter-filtres **plus √©lev√©e** ‚Üí les filtres se sp√©cialisent sur des variations d'un m√™me concept\n",
    "- Certains filtres semblent ¬´ bruit√©s ¬ª ‚Üí ce n'est pas du bruit, c'est un **pattern trop abstrait** pour notre perception\n",
    "\n",
    "#### Impact du learning rate sur l'entra√Ænement\n",
    "\n",
    "| LR | Effet sur les filtres | Diagnostic |\n",
    "|----|----------------------|------------|\n",
    "| Trop √©lev√© | Filtres bruit√©s, pas de structure ‚Üí oscillation des poids | ¬´ Salt & pepper ¬ª pattern |\n",
    "| Optimal | Filtres nets, diversifi√©s, hi√©rarchis√©s | Structure claire √† chaque couche |\n",
    "| Trop bas | Filtres redondants ou ¬´ morts ¬ª ‚Üí convergence insuffisante | Beaucoup de filtres quasi-identiques |\n",
    "\n",
    "#### Transf√©rabilit√© des filtres\n",
    "\n",
    "Les filtres bas-niveau sont **quasi-universels** (bords, textures) : on les retrouve dans des r√©seaux entra√Æn√©s sur ImageNet, sur des visages, ou m√™me sur des images m√©dicales. C'est le fondement du **transfer learning** ‚Äî on peut r√©utiliser ces couches et ne r√©-entra√Æner que les couches hautes sur un nouveau domaine.\n",
    "\n",
    "> üìö Yosinski et al. (2014), *\"How transferable are features in deep neural networks?\"* ‚Äî les 3 premi√®res couches sont quasi-identiques entre r√©seaux entra√Æn√©s sur des t√¢ches diff√©rentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Partie 2 ‚Äî Grad-CAM, Occlusion & Anonymisation\n",
    "\n",
    "## 2.1 Grad-CAM : th√©orie\n",
    "\n",
    "**Grad-CAM** produit une carte de chaleur des r√©gions qui influencent la d√©cision du r√©seau. Contrairement √† la visualisation des filtres (Partie 1) qui montre ce que le r√©seau *peut* d√©tecter, le Grad-CAM montre ce qu'il *utilise effectivement* pour une image donn√©e.\n",
    "\n",
    "**Formulation math√©matique** :\n",
    "\n",
    "1. On calcule les poids d'importance $\\alpha_k^c$ pour chaque feature map $A^k$ :\n",
    "$$\\alpha_k^c = \\underbrace{\\frac{1}{Z} \\sum_{i} \\sum_{j}}_{\\text{Global Average Pooling}} \\frac{\\partial y^c}{\\partial A^k_{ij}}$$\n",
    "\n",
    "2. La carte d'activation est :\n",
    "$$L^c_{\\text{Grad-CAM}} = \\text{ReLU}\\left(\\sum_k \\alpha_k^c \\cdot A^k\\right)$$\n",
    "\n",
    "Le **ReLU** ne conserve que les activations *positives* : on veut les r√©gions qui **contribuent** √† la classe, pas celles qui la **contredisent**.\n",
    "\n",
    "> Ref: Selvaraju et al. (2017), [*\"Grad-CAM: Visual Explanations from Deep Networks\"*](https://arxiv.org/abs/1610.02391)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Impl√©mentation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "IMG_PATH = \"test_face.jpg\"\n",
    "\n",
    "def load_image(path, size=(224, 224)):\n",
    "    img = cv2.imread(path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, size)\n",
    "    x = keras.applications.vgg16.preprocess_input(\n",
    "        np.expand_dims(img.astype('float32').copy(), 0)\n",
    "    )\n",
    "    return img, x\n",
    "\n",
    "def make_gradcam_heatmap(img_array, model, layer_name, pred_index=None):\n",
    "    grad_model = keras.Model(\n",
    "        inputs=model.input,\n",
    "        outputs=[model.get_layer(layer_name).output, model.output]\n",
    "    )\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_out, preds = grad_model(img_array)\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(preds[0])\n",
    "        class_channel = preds[:, pred_index]\n",
    "    grads = tape.gradient(class_channel, conv_out)\n",
    "    pooled = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "    heatmap = tf.squeeze(conv_out[0] @ pooled[..., tf.newaxis])\n",
    "    heatmap = tf.maximum(heatmap, 0) / (tf.reduce_max(heatmap) + 1e-8)\n",
    "    return heatmap.numpy()\n",
    "\n",
    "def overlay_heatmap(img, heatmap, alpha=0.4):\n",
    "    h = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "    h_color = cv2.applyColorMap(np.uint8(255 * h), cv2.COLORMAP_JET)\n",
    "    h_color = cv2.cvtColor(h_color, cv2.COLOR_BGR2RGB)\n",
    "    return (h_color * alpha + img * (1 - alpha)).astype(np.uint8)\n",
    "\n",
    "img_rgb, img_input = load_image(IMG_PATH)\n",
    "print(\"‚úÖ Fonctions d√©finies, image charg√©e\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Pr√©dictions ImageNet\n",
    "preds = model.predict(img_input, verbose=0)\n",
    "top5 = keras.applications.vgg16.decode_predictions(preds, top=5)[0]\n",
    "print(\"Top 5 pr√©dictions ImageNet :\")\n",
    "for i, (_, label, score) in enumerate(top5):\n",
    "    bar = \"‚ñà\" * int(score * 50)\n",
    "    print(f\"  {i+1}. {label:20s} {score:.4f} {bar}\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è  Le mod√®le est entra√Æn√© sur ImageNet (objets), pas VGGFace (visages).\")\n",
    "print(\"   ‚Üí Il classifie le v√™tement (jersey), pas l'identit√© de la personne.\")\n",
    "print(\"   ‚Üí C'est un point cl√© pour l'interpr√©tation du Grad-CAM ci-dessous.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Grad-CAM multi-couches"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "cam_layers = ['block3_conv3', 'block4_conv3', 'block5_conv3']\n",
    "\n",
    "fig, axes = plt.subplots(2, len(cam_layers) + 1, figsize=(20, 10))\n",
    "\n",
    "# Row 1: images + overlays\n",
    "axes[0, 0].imshow(img_rgb)\n",
    "axes[0, 0].set_title(\"Original\", fontsize=12, fontweight='bold')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "heatmaps = {}\n",
    "for i, layer in enumerate(cam_layers):\n",
    "    hm = make_gradcam_heatmap(img_input, model, layer)\n",
    "    heatmaps[layer] = hm\n",
    "    axes[0, i+1].imshow(overlay_heatmap(img_rgb, hm))\n",
    "    axes[0, i+1].set_title(f\"Grad-CAM: {layer}\", fontsize=12, fontweight='bold')\n",
    "    axes[0, i+1].axis('off')\n",
    "\n",
    "# Row 2: raw heatmaps (pour analyse quantitative)\n",
    "axes[1, 0].text(0.5, 0.5, \"Heatmaps\\nbrutes\\n(sans overlay)\", \n",
    "                ha='center', va='center', fontsize=12, transform=axes[1,0].transAxes)\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "for i, layer in enumerate(cam_layers):\n",
    "    im = axes[1, i+1].imshow(heatmaps[layer], cmap='jet', vmin=0, vmax=1)\n",
    "    axes[1, i+1].set_title(f\"Raw: {layer}\", fontsize=11)\n",
    "    axes[1, i+1].axis('off')\n",
    "    plt.colorbar(im, ax=axes[1, i+1], fraction=0.046)\n",
    "\n",
    "plt.suptitle(\"Grad-CAM ‚Äî √âvolution de l'attention √† travers les couches\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"output_figures/02_gradcam.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Analyse quantitative du Grad-CAM"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# D√©finir des r√©gions d'int√©r√™t (ROI) sur le visage\n",
    "rois = {\n",
    "    \"Front/Cheveux\": (0, 0.25, 0, 1),      # (y1%, y2%, x1%, x2%)\n",
    "    \"Yeux\":          (0.25, 0.42, 0.1, 0.9),\n",
    "    \"Nez\":           (0.42, 0.58, 0.25, 0.75),\n",
    "    \"Bouche/Menton\": (0.58, 0.78, 0.15, 0.85),\n",
    "    \"Cou/V√™tement\":  (0.78, 1.0, 0, 1),\n",
    "}\n",
    "\n",
    "print(\"=\" * 75)\n",
    "print(f\"{'R√©gion':20s}\", end=\"\")\n",
    "for layer in cam_layers:\n",
    "    print(f\" | {layer:>15s}\", end=\"\")\n",
    "print(\"\\n\" + \"-\" * 75)\n",
    "\n",
    "for roi_name, (y1, y2, x1, x2) in rois.items():\n",
    "    print(f\"{roi_name:20s}\", end=\"\")\n",
    "    for layer in cam_layers:\n",
    "        hm = heatmaps[layer]\n",
    "        h, w = hm.shape\n",
    "        roi_val = hm[int(h*y1):int(h*y2), int(w*x1):int(w*x2)].mean()\n",
    "        bar = \"‚ñà\" * int(roi_val * 20)\n",
    "        print(f\" | {roi_val:>6.3f} {bar:10s}\", end=\"\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 75)\n",
    "print(\"\\n‚Üí Valeurs plus √©lev√©es = la r√©gion contribue plus √† la classification\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse critique des r√©sultats Grad-CAM\n",
    "\n",
    "#### Observation principale : le biais ImageNet\n",
    "\n",
    "Le mod√®le pr√©dit **\"jersey\"** (v√™tement), pas une identit√© faciale. Le Grad-CAM refl√®te ce biais :\n",
    "- **block5_conv3** montre une forte activation sur le **cou et le t-shirt** ‚Äî coh√©rent avec la classe pr√©dite\n",
    "- **block3_conv3** diffuse l'attention sur le **visage et le col** ‚Äî les textures du tissu ET de la peau sont capt√©es\n",
    "- Le r√©seau ne ¬´ regarde ¬ª pas le visage pour identifier une personne, mais pour classifier un objet\n",
    "\n",
    "#### Lien avec la Partie 1\n",
    "\n",
    "La Partie 1 nous a montr√© que les filtres de `block5` d√©tectent des **structures abstraites**. Le Grad-CAM r√©v√®le maintenant *lesquels* de ces filtres s'activent sur cette image. C'est compl√©mentaire :\n",
    "- **Partie 1** = *que peut d√©tecter le r√©seau ?* (capacit√©)\n",
    "- **Partie 2** = *que d√©tecte-t-il effectivement ?* (utilisation)\n",
    "\n",
    "#### Implication pour un mod√®le VGGFace\n",
    "\n",
    "Avec un mod√®le entra√Æn√© sur des visages (VGGFace), le Grad-CAM montrerait une attention concentr√©e sur les **traits du visage discriminants** (yeux, nez, bouche), pas sur les v√™tements. Cette diff√©rence illustre que **l'interpr√©tabilit√© d√©pend fortement du domaine d'entra√Ænement**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Occluding Parts ‚Äî Analyse par occlusion syst√©matique\n",
    "\n",
    "L'analyse par occlusion est compl√©mentaire au Grad-CAM : au lieu de calculer des gradients, on **masque physiquement** une zone de l'image et on mesure la chute de confiance. C'est plus co√ªteux mais plus intuitif et ne d√©pend pas d'hypoth√®ses de lin√©arit√©.\n",
    "\n",
    "**Protocole** : un patch gris (128, 128, 128) de 25√ó25 px glisse sur l'image avec un stride de 14 px. Pour chaque position, on mesure la baisse de score de la classe pr√©dite."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def occlusion_sensitivity(img_path, model, patch=25, stride=14):\n",
    "    img, x = load_image(img_path)\n",
    "    base_pred = model.predict(x, verbose=0)\n",
    "    top_class = np.argmax(base_pred[0])\n",
    "    base_score = base_pred[0][top_class]\n",
    "    \n",
    "    h, w = 224, 224\n",
    "    smap, count = np.zeros((h, w)), np.zeros((h, w))\n",
    "    \n",
    "    for y in range(0, h - patch, stride):\n",
    "        for x_pos in range(0, w - patch, stride):\n",
    "            occ = img.copy()\n",
    "            occ[y:y+patch, x_pos:x_pos+patch] = 128\n",
    "            occ_input = keras.applications.vgg16.preprocess_input(\n",
    "                np.expand_dims(occ.astype('float32'), 0)\n",
    "            )\n",
    "            score = model.predict(occ_input, verbose=0)[0][top_class]\n",
    "            smap[y:y+patch, x_pos:x_pos+patch] += base_score - score\n",
    "            count[y:y+patch, x_pos:x_pos+patch] += 1\n",
    "    \n",
    "    count[count == 0] = 1\n",
    "    return smap / count, base_score\n",
    "\n",
    "print(\"‚è≥ Analyse par occlusion (~1-2 min sur CPU)...\")\n",
    "smap, base_score = occlusion_sensitivity(IMG_PATH, model, patch=25, stride=14)\n",
    "print(f\"‚úÖ Score de base: {base_score:.4f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "axes[0].imshow(img_rgb)\n",
    "axes[0].set_title(\"Image originale\", fontweight='bold', fontsize=12)\n",
    "axes[0].axis('off')\n",
    "\n",
    "im = axes[1].imshow(smap, cmap='hot', interpolation='bilinear')\n",
    "axes[1].set_title(\"Carte de sensibilit√© (Œî confiance)\", fontweight='bold', fontsize=12)\n",
    "axes[1].axis('off')\n",
    "plt.colorbar(im, ax=axes[1], fraction=0.046, label=\"Chute de score\")\n",
    "\n",
    "axes[2].imshow(img_rgb, alpha=0.6)\n",
    "axes[2].imshow(smap, cmap='hot', alpha=0.5, interpolation='bilinear')\n",
    "axes[2].set_title(\"Superposition\", fontweight='bold', fontsize=12)\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.suptitle(\"Occluding Parts ‚Äî Quelles zones sont critiques pour la classification ?\", fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"output_figures/03_occlusion.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Analyse par zone de la carte de sensibilit√©\n",
    "print(\"Sensibilit√© par zone anatomique :\")\n",
    "print(\"=\" * 55)\n",
    "for roi_name, (y1, y2, x1, x2) in rois.items():\n",
    "    h, w = smap.shape\n",
    "    roi_val = smap[int(h*y1):int(h*y2), int(w*x1):int(w*x2)]\n",
    "    mean_s = roi_val.mean()\n",
    "    max_s = roi_val.max()\n",
    "    bar = \"‚ñà\" * int(abs(mean_s) / (abs(smap).max() + 1e-8) * 20)\n",
    "    print(f\"  {roi_name:20s} | mean={mean_s:>+.4f} | max={max_s:>+.4f} | {bar}\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"\\nSensibilit√© globale max: {smap.max():.4f}\")\n",
    "print(f\"Zone la plus sensible: {max(rois.items(), key=lambda x: smap[int(224*x[1][0]):int(224*x[1][1]), int(224*x[1][2]):int(224*x[1][3])].mean())[0]}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparaison Grad-CAM vs Occlusion\n",
    "\n",
    "| M√©thode | Avantages | Limites |\n",
    "|---------|-----------|---------|\n",
    "| **Grad-CAM** | Rapide (1 forward + 1 backward), r√©solution de la feature map | D√©pend de la lin√©arit√© locale, 1 seule couche |\n",
    "| **Occlusion** | Model-agnostic, intuitif, mesure directe de l'impact | Lent (N¬≤ forward passes), r√©solution du patch |\n",
    "\n",
    "Les deux m√©thodes convergent sur les m√™mes conclusions ‚Äî ce qui renforce la fiabilit√© de l'analyse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Anonymisation CNIL ‚Äî Exp√©rimentation multi-masques\n",
    "\n",
    "La CNIL recommande le masquage des yeux pour l'anonymisation. Testons syst√©matiquement **3 niveaux** de masquage pour quantifier leur efficacit√© :"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def apply_mask(img, mask_type):\n",
    "    masked = img.copy()\n",
    "    h = 224\n",
    "    if mask_type == \"yeux\":\n",
    "        masked[int(h*0.28):int(h*0.42), :] = 0\n",
    "    elif mask_type == \"yeux+nez\":\n",
    "        masked[int(h*0.28):int(h*0.58), :] = 0\n",
    "    elif mask_type == \"visage_complet\":\n",
    "        masked[int(h*0.15):int(h*0.80), int(h*0.10):int(h*0.90)] = 0\n",
    "    elif mask_type == \"flou_gaussien\":\n",
    "        face_region = masked[int(h*0.10):int(h*0.85), int(h*0.05):int(h*0.95)]\n",
    "        blurred = cv2.GaussianBlur(face_region, (51, 51), 30)\n",
    "        masked[int(h*0.10):int(h*0.85), int(h*0.05):int(h*0.95)] = blurred\n",
    "    return masked\n",
    "\n",
    "mask_types = [\"yeux\", \"yeux+nez\", \"visage_complet\", \"flou_gaussien\"]\n",
    "fig, axes = plt.subplots(1, len(mask_types) + 1, figsize=(22, 5))\n",
    "\n",
    "# Original\n",
    "pred_orig = model.predict(img_input, verbose=0)\n",
    "orig_top = keras.applications.vgg16.decode_predictions(pred_orig, top=1)[0][0]\n",
    "axes[0].imshow(img_rgb)\n",
    "axes[0].set_title(f\"Original\\n{orig_top[1]}: {orig_top[2]:.3f}\", fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "print(\"R√©sultats multi-masques :\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  {'Masque':20s} | {'Top classe':15s} | {'Score':>8s} | {'Chute':>8s}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for i, mt in enumerate(mask_types):\n",
    "    masked = apply_mask(img_rgb, mt)\n",
    "    m_input = keras.applications.vgg16.preprocess_input(\n",
    "        np.expand_dims(masked.astype('float32'), 0)\n",
    "    )\n",
    "    m_pred = model.predict(m_input, verbose=0)\n",
    "    m_top = keras.applications.vgg16.decode_predictions(m_pred, top=1)[0][0]\n",
    "    drop = orig_top[2] - m_top[2]\n",
    "    \n",
    "    axes[i+1].imshow(masked)\n",
    "    axes[i+1].set_title(f\"{mt}\\n{m_top[1]}: {m_top[2]:.3f}\", fontweight='bold')\n",
    "    axes[i+1].axis('off')\n",
    "    \n",
    "    print(f\"  {mt:20s} | {m_top[1]:15s} | {m_top[2]:>8.4f} | {drop:>+8.4f}\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "plt.suptitle(\"Efficacit√© compar√©e des m√©thodes d'anonymisation\", fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"output_figures/04_cnil_mask.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Discussion ‚Äî Anonymisation & Biais Ethnique\n",
    "\n",
    "#### Le masque sur les yeux est-il suffisant ?\n",
    "\n",
    "Nos r√©sultats exp√©rimentaux montrent clairement que **non** :\n",
    "- Le masque ¬´ yeux seuls ¬ª provoque une **chute de confiance faible** ‚Äî le r√©seau continue √† classifier correctement\n",
    "- Le Grad-CAM confirme que le r√©seau utilise **nez, bouche, m√¢choire, texture de la peau, et m√™me les v√™tements**\n",
    "- Seul le masquage du **visage complet** ou le **flou gaussien** d√©gradent significativement la classification\n",
    "\n",
    "#### Le biais ethnique : un probl√®me syst√©mique\n",
    "\n",
    "L'√©tude de **Shrutin et al. (2019)**, *\"Deep Learning for Face Recognition: Pride or Prejudiced?\"*, r√©v√®le un ph√©nom√®ne alarmant :\n",
    "\n",
    "1. **R√©partition des features discriminantes** : pour les sujets caucasiens, le r√©seau concentre ~60% de son attention sur les yeux. Pour d'autres ethnicit√©s, la distribution est plus diffuse (nez, bouche, texture)\n",
    "2. **Cons√©quence directe** : le masquage des yeux \"anonymise\" efficacement les caucasiens mais **pas les autres populations**\n",
    "3. **Cause racine** : les datasets d'entra√Ænement (LFW, VGGFace) sont d√©s√©quilibr√©s (>70% caucasiens) ‚Üí le r√©seau apprend des raccourcis biais√©s\n",
    "\n",
    "#### Proposition d'anonymisation robuste\n",
    "\n",
    "| M√©thode | R√©sistance CNN | Commodit√© | Recommandation |\n",
    "|---------|:-:|:-:|---------------|\n",
    "| Masque yeux | ‚ùå Faible | ‚úÖ Simple | Insuffisante |\n",
    "| Masque yeux+nez+bouche | ‚ö†Ô∏è Moyenne | ‚ö†Ô∏è Moyen | Minimum viable |\n",
    "| Flou gaussien visage | ‚úÖ Forte | ‚úÖ Simple | **Recommand√©e** |\n",
    "| Remplacement par avatar | ‚úÖ Forte | ‚ùå Complexe | Id√©ale si faisable |\n",
    "\n",
    "> **Recommandation** : le flou gaussien de l'ensemble du visage est le meilleur compromis efficacit√©/simplicit√©. Il d√©truit les features **√† tous les niveaux** du CNN (bords, textures, structures)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Partie 3 ‚Äî Syst√®me de Reconnaissance Faciale One-Shot\n",
    "\n",
    "## 3.1 Architecture & Workflow\n",
    "\n",
    "**Contrainte** : reconnaissance avec **1 seule photo** par personne (one-shot learning)\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                     ENREGISTREMENT                           ‚îÇ\n",
    "‚îÇ                                                              ‚îÇ\n",
    "‚îÇ  Photo ‚îÄ‚îÄ‚Üí D√©tection ‚îÄ‚îÄ‚Üí Resize ‚îÄ‚îÄ‚Üí VGG16 ‚îÄ‚îÄ‚Üí Embedding ‚îÄ‚îÄ‚Üí DB  ‚îÇ\n",
    "‚îÇ            visage        224√ó224    (frozen)   R‚Åø            ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                     RECONNAISSANCE                           ‚îÇ\n",
    "‚îÇ                                                              ‚îÇ\n",
    "‚îÇ  Photo ‚îÄ‚îÄ‚Üí m√™me pipeline ‚îÄ‚îÄ‚Üí Embedding ‚îÄ‚îÄ‚Üí Cosine Sim. ‚îÄ‚îÄ‚Üí ID   ‚îÇ\n",
    "‚îÇ                               requ√™te      vs DB        ou ‚ùå    ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "**Choix techniques justifi√©s** :\n",
    "- **VGG16 block5_pool** comme extracteur : produit un embedding de 25 088 dimensions (7√ó7√ó512)\n",
    "- **Cosine similarity** plut√¥t qu'euclidienne : invariant √† la norme, plus robuste aux variations d'√©clairage\n",
    "- **Seuil de rejet** : en dessous d'un seuil, on refuse l'identification (s√©curit√©)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Impl√©mentation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "feature_extractor = keras.Model(\n",
    "    inputs=model.input,\n",
    "    outputs=model.get_layer('block5_pool').output\n",
    ")\n",
    "print(f\"Extracteur: {model.input_shape} ‚Üí {feature_extractor.output_shape}\")\n",
    "print(f\"Dimension embedding: {np.prod(feature_extractor.output_shape[1:]):,}\")\n",
    "\n",
    "\n",
    "class FaceRecognizer:\n",
    "    def __init__(self, extractor, threshold=0.5):\n",
    "        self.extractor = extractor\n",
    "        self.threshold = threshold\n",
    "        self.db = {}\n",
    "    \n",
    "    def _embed(self, img_path):\n",
    "        _, x = load_image(img_path)\n",
    "        feat = self.extractor.predict(x, verbose=0).flatten()\n",
    "        return feat / (np.linalg.norm(feat) + 1e-8)\n",
    "    \n",
    "    def register(self, name, img_path):\n",
    "        self.db[name] = self._embed(img_path)\n",
    "        print(f\"  ‚úÖ '{name}' enregistr√© (dim={len(self.db[name])})\")\n",
    "    \n",
    "    def identify(self, img_path):\n",
    "        if not self.db:\n",
    "            return \"DB vide\", 0.0\n",
    "        query = self._embed(img_path)\n",
    "        best = max(self.db.items(), key=lambda item: np.dot(query, item[1]))\n",
    "        sim = np.dot(query, best[1])\n",
    "        status = \"‚úÖ Identifi√©\" if sim >= self.threshold else \"‚ùå Rejet√©\"\n",
    "        return best[0], sim, status\n",
    "\n",
    "# D√©monstration\n",
    "recognizer = FaceRecognizer(feature_extractor)\n",
    "recognizer.register(\"Personne Test\", IMG_PATH)\n",
    "\n",
    "name, score, status = recognizer.identify(IMG_PATH)\n",
    "print(f\"\\nüîç Test self-identification:\")\n",
    "print(f\"   R√©sultat: {name} | Cosine sim: {score:.6f} | {status}\")\n",
    "print(f\"\\n   ‚Üí sim=1.0 attendue (m√™me image) : {'‚úÖ Correct' if score > 0.999 else '‚ö†Ô∏è Inattendu'}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Analyse ‚Äî Pourquoi KNN et pas un r√©seau de neurones ?\n",
    "\n",
    "#### Le dilemme du one-shot\n",
    "\n",
    "Un classificateur NN classique n√©cessite **des centaines d'exemples** par classe pour apprendre les fronti√®res de d√©cision. Avec une seule image par personne, il **overfitterait** imm√©diatement.\n",
    "\n",
    "Le **KNN (K=1)** n'a pas ce probl√®me car il ne ¬´ s'entra√Æne ¬ª pas ‚Äî il compare directement les embeddings dans l'espace des features.\n",
    "\n",
    "#### Pourquoi √ßa fonctionne\n",
    "\n",
    "Le succ√®s repose enti√®rement sur la **qualit√© de l'espace d'embedding** :\n",
    "1. VGG16 (pr√©-entra√Æn√©) projette les images dans un espace o√π les **distances s√©mantiques** sont pr√©serv√©es\n",
    "2. Deux photos de la m√™me personne ‚Üí embeddings proches (haute cosine sim.)\n",
    "3. Photos de personnes diff√©rentes ‚Üí embeddings √©loign√©s\n",
    "\n",
    "C'est du **metric learning implicite** : le r√©seau n'a pas √©t√© entra√Æn√© explicitement pour la similarit√©, mais les features de haut niveau capturent naturellement l'identit√©.\n",
    "\n",
    "#### Limites et am√©liorations\n",
    "\n",
    "| Limite | Solution |\n",
    "|--------|----------|\n",
    "| VGG16 ImageNet ‚â† visages | Utiliser VGGFace ou ArcFace |\n",
    "| Embedding 25K dims ‚Üí lent | PCA ou auto-encoder pour compression |\n",
    "| Pas robuste aux poses extr√™mes | Data augmentation √† l'enregistrement |\n",
    "| Seuil fixe | Seuil adaptatif par personne |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Partie 4 ‚Äî Production & D√©ploiement\n",
    "\n",
    "## 4.1 Export du mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "save_path = \"saved_model/face_features\"\n",
    "feature_extractor.export(save_path)\n",
    "\n",
    "import subprocess\n",
    "result = subprocess.run(['du', '-sh', save_path], capture_output=True, text=True)\n",
    "print(f\"‚úÖ Mod√®le export√©: {result.stdout.strip()}\")\n",
    "print(f\"   Format: TensorFlow SavedModel (.pb + variables)\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Contraintes de production\n",
    "\n",
    "| Contrainte | VGG16 brut | Apr√®s optimisation |\n",
    "|------------|:----------:|:------------------:|\n",
    "| **Taille mod√®le** | ~528 MB | ~60 MB (quantization INT8) |\n",
    "| **Latence CPU** | ~200ms | ~50ms (TF Lite + XNNPACK) |\n",
    "| **Latence GPU** | ~15ms | ~5ms (TensorRT) |\n",
    "| **RAM inference** | ~1.5 GB | ~200 MB |\n",
    "\n",
    "#### Options de d√©ploiement\n",
    "\n",
    "1. **Serveur (TF Serving)** : API REST/gRPC, batch processing, scalable\n",
    "2. **Mobile (TF Lite)** : quantification INT8, d√©l√©gu√© GPU, ~60 MB\n",
    "3. **Navigateur (TF.js)** : WebGL backend, pas de serveur, ~15 MB (apr√®s pruning)\n",
    "4. **Embarqu√© (C++ / OpenCV DNN)** : performance native, pas de d√©pendances Python\n",
    "\n",
    "#### S√©curit√© & RGPD\n",
    "\n",
    "- Les **embeddings faciaux sont des donn√©es biom√©triques** ‚Üí Article 9 RGPD\n",
    "- Chiffrement obligatoire au repos et en transit\n",
    "- Droit √† l'effacement : supprimer l'embedding = \"oublier\" une personne\n",
    "- **Pas de stockage des images originales** en production, uniquement les embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Conclusion & Perspectives\n",
    "\n",
    "## Synth√®se des r√©sultats\n",
    "\n",
    "| Question | M√©thode utilis√©e | R√©ponse |\n",
    "|----------|-----------------|---------|\n",
    "| Que d√©tectent les filtres CNN ? | Maximisation des activations | Hi√©rarchie bords ‚Üí textures ‚Üí structures |\n",
    "| Quelles zones influencent la d√©cision ? | Grad-CAM + Occlusion | D√©pend du domaine d'entra√Ænement (ImageNet ‚â† VGGFace) |\n",
    "| Le masque yeux suffit-il ? | Multi-masques + analyse quantitative | **Non** ‚Äî seul le flou complet est robuste |\n",
    "| KNN vs NN en one-shot ? | Analyse th√©orique + impl√©mentation | KNN + transfer learning est optimal |\n",
    "\n",
    "## Ce que ce TP r√©v√®le sur les CNNs\n",
    "\n",
    "1. **Les CNNs ne sont pas des bo√Ætes noires** ‚Äî on dispose d'outils (filter viz, Grad-CAM, occlusion) pour comprendre leurs d√©cisions\n",
    "2. **L'interpr√©tabilit√© est un outil pratique** ‚Äî elle permet de questionner des pratiques r√©elles (CNIL) avec des preuves visuelles\n",
    "3. **Le domaine d'entra√Ænement est d√©terminant** ‚Äî un m√™me r√©seau (VGG16) ¬´ regarde ¬ª des zones diff√©rentes selon qu'il a appris des objets ou des visages\n",
    "4. **Le biais est structurel** ‚Äî il vient des donn√©es, pas de l'algorithme, et se propage silencieusement dans les recommandations r√©glementaires\n",
    "\n",
    "## Pour aller plus loin\n",
    "\n",
    "- Comparer les Grad-CAM de VGG16-ImageNet vs VGGFace sur la m√™me image\n",
    "- Tester avec [ArcFace](https://arxiv.org/abs/1801.07698) ‚Äî l'√©tat de l'art en reconnaissance faciale\n",
    "- Ajouter l'analyse SHAP pour une attribution pixel-level\n",
    "- Impl√©menter un test A/B multi-ethnicit√© pour quantifier le biais"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}